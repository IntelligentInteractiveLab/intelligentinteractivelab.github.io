<!DOCTYPE html> <html lang="cn"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> RavenGaze | Intelligent Interaction Laboratory </title> <meta name="author" content="Intelligent Interaction Laboratory"> <meta name="description" content="Gaze Estimation Dataset Evoked by Raven Progressive Matrices (RPM) Test"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://intelligentinteractivelab.github.io/datasets/RavenGaze/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="//"> <span class="font-weight-bold">Intelligent Interaction</span> Laboratory </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/people/">People </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">Teaching </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/books/">bookshelf</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/blog/">blog</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">RavenGaze</h1> <p class="post-description">Gaze Estimation Dataset Evoked by Raven Progressive Matrices (RPM) Test</p> </header> <article> <div class="row justify-content-sm-center"> <div class="col-sm-10 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/expriment_process-480.webp 480w,/assets/img/expriment_process-800.webp 800w,/assets/img/expriment_process-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/expriment_process.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="expriment process" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h1 id="abstract">Abstract</h1> <p>One major challenge in appearance-based gaze estimation is the lack of high-quality labeled data. Establishing databases or datasets is a way to obtain accurate gaze data and test methods or tools. However, the methods of collecting data in existing databases are designed on artificial chasing target tasks or unintentional free-looking tasks, which are not natural and real eye interactions and cannot reflect the inner cognitive processes of humans. To fill this gap, we propose the first gaze estimation dataset collected from an actual psychological experiment by the eye tracker, called the RavenGaze dataset. We design an experiment employing Raven‚Äôs Matrices as visual stimuli and collecting gaze data, facial videos as well as screen content videos simultaneously. Thirty-four participants were recruited. The results show that the existing algorithms perform well on our RavenGaze dataset in the 3D and 2D gaze estimation task, and demonstrate good generalization ability according to cross-dataset evaluation task. RavenGaze and the establishment of the benchmark lay the foundation for other researchers to do further in-depth research and test their methods or tools.</p> <h2 id="description">Description</h2> <h3 id="stimuli-and-experiment">Stimuli and Experiment</h3> <p>We used the 48-question from Raven Progressive Matrices (RPM) as the subject‚Äôs visual stimuli. In addition, to ensure subjects are not distracted as much as possible during the task time, we set answer time for questions(15s) shorter than the average time to work out the questions. The procedure of the experiment is shown in Fig.1.</p> <div class="row justify-content-sm-center"> <div class="col-sm-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/1-experiment_procedure-480.webp 480w,/assets/img/1-experiment_procedure-800.webp 800w,/assets/img/1-experiment_procedure-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/1-experiment_procedure.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="1-experiment_procedure" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Fig. 1. The procedure of experiment </div> <p>Before each subject starts the investigation, first, the eye tracker is calibrated the gaze of the current subject to reduce the gaze error of the collection process. Then ten landscape pictures help the subject calm down and enter the test state. After that, it begins the test, which includes 48 questions. We set each question to have a maximum answering time of 15 seconds. The subject must press the button corresponding to the answer option preset by the program within the answering time to complete the question and automatically jump to the next question. If the subject does not meet the answer within the set time, it will automatically jump to the next question. Before jumping to the next question, a calibration break is set to ensure high-quality gaze data. It is a fixation point that appeared in the center of the screen for 0.5 seconds, and subjects were asked to focus on this fixation point as much as possible.During the answering process, the program simultaneously records the facial video, screen video, and eye movement data of each subject during completing the 48 reasoning questions.</p> <div class="row justify-content-sm-center"> <div class="col-sm-10 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2-Experiment_setting-480.webp 480w,/assets/img/2-Experiment_setting-800.webp 800w,/assets/img/2-Experiment_setting-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/2-Experiment_setting.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="1-2-Experiment_setting" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Fig. 2. Experiment setting </div> <p>The experiment setting is shown in Fig. 2, and the visual stimuli materials are displayed on a 27-Inch monitor. The subjects‚Äô videos are captured from a commercial webcam fixed on the top of the screen. Logitech C270 HD Webcam is chosen, since it is one of low-cost and most widely used webcam, supporting 1280 √ó 720 video recording with lightcorrection technology.</p> <h3 id="dataset-summary">Dataset Summary</h3> <p>We collected gaze data from a total of 34 subjects(18 female and 16 male), with 22 wearing glasses and 12 having normal vision. Three subjects‚Äô gaze data were screened out due to improper experimental operation. The data were collected under well-lit indoor conditions, where the sampling rate of face video was 30Hz, and the sampling rate of Tobii Pro Nano eye tracker was 60Hz. The time for each subject to complete RMP test is about 8-11 minutes. The entire dataset contains 31 subjects‚Äô facial videos with a total length of 309 minutes. There are 556,476 images in total, some face image examples are shown in Fig.3.</p> <div class="row justify-content-sm-center"> <div class="col-sm-10 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/3-face_image-480.webp 480w,/assets/img/3-face_image-800.webp 800w,/assets/img/3-face_image-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/3-face_image.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="3-face_image" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Fig. 3. Face image from the dataset </div> <div class="row justify-content-sm-center"> <div class="col-sm-10 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/4-densityHeatMap-480.webp 480w,/assets/img/4-densityHeatMap-800.webp 800w,/assets/img/4-densityHeatMap-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/4-densityHeatMap.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="4-densityHeatMap" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Fig. 4. Gaze angle and gaze point distribution </div> <p>The angle range of the horizontal direction is (-10¬∞, +10¬∞), and the angle of the vertical direction is (-12¬∞, 12¬∞), which is smaller than another gaze dataset. It also means that RavenGaze places higher demands on the recognition algorithms.</p> <h2 id="how-to-use">How to use</h2> <p>If you are interested in using this dataset, you will have to print, sign and scan an EULA (End User License Agreement) and upload it via the dataset request form. We will then supply you with a username and password to download the data.</p> <h2 id="publications">Publications</h2> <p>Publications include not only papers, but also presentations for conferences or educational purposes.All documents and papers that report on research that uses the RavenGaze dataset will acknowledge this by</p> <p>citing the following paper:</p> <blockquote> <p>T. Xu, B. Wu, Y. Bai and Y. Zhou, ‚ÄúRavenGaze: A Dataset for Gaze Estimation Leveraging Psychological Experiment Through Eye Tracker,‚Äù 2023 IEEE 17th International Conference on Automatic Face and Gesture Recognition (FG), Waikoloa Beach, HI, USA, 2023, pp. 1-6, doi: <a href="https://doi.org/10.1109/FG57933.2023.10042793" rel="external nofollow noopener" target="_blank">10.1109/FG57933.2023.10042793</a>.</p> </blockquote> <h2 id="credits">Credits</h2> <p>First and foremost we‚Äôd like to thank the all(34) participants in this study for having the patience and goodwill to let us record their data. This dataset was collected by: Intelligent Interaction Laboratory @ Northwestern Polytechnical University</p> <h2 id="dataset-access">Dataset access</h2> <p>To gain access to the dataset and download the files on this page, please download the license agreement below. The license agreement should be printed, signed, scanned and returned via email to <code class="language-plaintext highlighter-rouge">&lt;a href="mailto:xutao@nwpu.edu.cn"&gt;</code>xutao@nwpu.edu.cn<code class="language-plaintext highlighter-rouge">&lt;/a&gt;</code> with the subject of ‚ÄúRavenGaze account request‚Äù. Upon receipt, a username, a password and a download link will be sent to download the data files below.</p> <p><a href="/assets/pdf/license_RavenGaze.pdf">The License Agreement</a></p> <script async="" src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script> <center> Views count:<span id="busuanzi_value_site_pv"><i class="fa fa-spinner fa-spin"></i></span>üëÄ | Number of visitors:<span id="busuanzi_value_site_uv"><i class="fa fa-spinner fa-spin"></i></span>üë¶ </center> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> ¬© Copyright 2025 Intelligent Interaction Laboratory. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>